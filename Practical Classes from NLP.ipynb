{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34245496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab22c014",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce34cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waxaan', 'ahay', 'Arday', '.', 'Waxaan', 'dhigtaa', 'Jaamacadda', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Word Tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Waxaan ahay Arday. Waxaan dhigtaa Jaamacadda.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d80c7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Waxaan ahay arday.',\n",
       " 'Waxaan dhigtaa Jaamacadda.',\n",
       " 'Waxaan daganahay Xaafadda Xawaadle']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Sentence Tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Waxaan ahay arday. Waxaan dhigtaa Jaamacadda. Waxaan daganahay Xaafadda Xawaadle\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4508693",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febbf211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'look', 'look', 'definit', 'watch', 'chang', 'one', 'start']\n"
     ]
    }
   ],
   "source": [
    "# Stemming in NLTK\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "words = ['first', 'time', 'sees', 'seconds', 'looking', 'look', 'definitely', 'watches', 'change', 'ones', 'started']\n",
    "stemmed_words = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2a6dbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'looking', 'look', 'definitely', 'watch', 'change', 'one', 'started']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization in NLTK\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmed_words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844812a",
   "metadata": {},
   "source": [
    "# Context Free Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a2c8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', 'girls', 'and', 'boys']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Context Free Grammar\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "\n",
    "    S -> NP Conj N | Adj NP\n",
    "    NP -> N Conj N | Adj N\n",
    "    Adj -> 'smart'\n",
    "    N -> 'girls' | 'boys'\n",
    "    Conj -> 'and'\n",
    "\n",
    "\"\"\")\n",
    "sent = \"smart girls and boys\".split()\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57942271",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = nltk.RecursiveDescentParser(grammar)\n",
    "parsed_sent = parser.parse(sent)\n",
    "for tree in parser.parse(sent):\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c85c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['smart', 'girls', 'and', 'boys']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mygrammar = nltk.PCFG.fromstring(\"\"\"\n",
    "\n",
    "    S -> NP Conj N [0.6]\n",
    "    S -> Adj NP [0.4]\n",
    "    NP -> N Conj N [0.5]\n",
    "    NP -> Adj N [0.5]\n",
    "    Adj -> 'smart' [1]\n",
    "    N -> 'girls' [0.5]\n",
    "    N -> 'boys' [0.5]\n",
    "    Conj -> 'and' [1]\n",
    "\n",
    "\"\"\")\n",
    "mysent = \"smart girls and boys\".split()\n",
    "mysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94f1ff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mygrammar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m myparser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mViterbiParser(\u001b[43mmygrammar\u001b[49m)\n\u001b[0;32m      2\u001b[0m myparsed_sent \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(mysent)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m myparser\u001b[38;5;241m.\u001b[39mparse(mysent):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mygrammar' is not defined"
     ]
    }
   ],
   "source": [
    "myparser = nltk.ViterbiParser(mygrammar)\n",
    "myparsed_sent = parser.parse(mysent)\n",
    "for tree in myparser.parse(mysent):\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae67cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geli Jumladda aad doonayso in aad ogaato: casho da kahor annigu waan seexday\n",
      "            J                              \n",
      "   _________|____________________           \n",
      "  |    |         EW              |         \n",
      "  |    |     ____|____           |          \n",
      "  MW   |    |         MW         FW        \n",
      "  |    |    |         |      ____|_____     \n",
      "  M    Q    E         M     E          F   \n",
      "  |    |    |         |     |          |    \n",
      "casho  da kahor     annigu waan     seexday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "                         ##############################################\n",
    "                         #    J = Jumlad       = Sentence             #\n",
    "                         #    M = Magac        = Noun                 #\n",
    "                         #    Y = Magac u yaal = Pronoun              #\n",
    "                         #    S = Sifo         = Adjective            #\n",
    "                         #    F = Ficil        = Verb                 #\n",
    "                         #    E = Meeleeye     = Proposition          #\n",
    "                         #    Q = Qodob        = Determinant (Article)#\n",
    "                         #    W = Weedh        = Phrase               #\n",
    "                         # 1. iyadu waxay aragtay suuqa weyn          #\n",
    "                         # 2. isagu wuxuu ku socday gaadhiga yar      #\n",
    "                         # 3. annigu suuqa waan tagay                 #\n",
    "                         # 4. iyadu wey socotay                       #\n",
    "                         # 5. casho da kahor annigu waan seexday      #\n",
    "                         ##############################################\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    J -> MW FW | EW FW | MW MW EW FW | EW EW FW | MW Q EW FW \n",
    "\n",
    "    SW -> S | S SW\n",
    "    MW -> M | M Q | M S | EW M\n",
    "    EW -> E | E MW\n",
    "    FW -> F | E F | E E F M S | E F M S \n",
    "\n",
    "    S -> \"weyn\" | \"cagaar\" | \"yar\" | \"qalayl\" | \"balaadhan\"\n",
    "    Q -> \"ka\" | \"ga\" | \"ha\" | \"a\" | \"ta\" | \"da\" | \"sha\"\n",
    "    M -> \"iyadu\" | \"annigu\" | \"suuqa\" | \"gaadhiga\" | \"wado\" | \"ey\" | \"isagu\" | \"casho\"\n",
    "    E -> \"waxay\" | \"wuxuu\" | \"wey\" | \"kadib\" | \"kahor\" | \"hoose\" | \"la\" | \"waan\" | \"wuu\" | \"ku\"\n",
    "    F -> \"aragtay\" | \"socday\" | \"maqashay\" | \"tagay\" | \"seexday\"\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "sentence = input(\"Geli Jumladda aad doonayso in aad ogaato: \").split()\n",
    "try:\n",
    "    for tree in parser.parse(sentence):\n",
    "        tree.pretty_print()\n",
    "        tree.draw()\n",
    "        break\n",
    "except ValueError:\n",
    "    print(\"No parse tree possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc321a",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43140796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('text', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('test', 'VB'),\n",
       " ('part', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('tagging', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('NLTK', 'NNP')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Tagger\n",
    "\n",
    "text=\"This is a text to test part of speech tagging in NLTK\"\n",
    "token= nltk.word_tokenize(text)\n",
    "nltk.pos_tag(token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32eac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Annigu', 'NN'),\n",
       " ('shalay', 'NN'),\n",
       " ('waxaan', 'NN'),\n",
       " ('tagay', 'NN'),\n",
       " ('Burco', 'NN'),\n",
       " ('oo', 'NN'),\n",
       " ('waxaan', 'NN'),\n",
       " ('booqday', 'NN'),\n",
       " ('saaxiibbaday', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default Tagger\n",
    "\n",
    "text1=\" Annigu shalay waxaan tagay Burco oo waxaan booqday saaxiibbaday \"\n",
    "token1= nltk.word_tokenize(text1)\n",
    "#nltk.pos_tag(token1) \n",
    "som_defalut_tagger=nltk.DefaultTagger('NN')\n",
    "som_defalut_tagger.tag(token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f31ac3",
   "metadata": {},
   "source": [
    "# NLTK N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fff0c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You',)\n",
      "('will',)\n",
      "('face',)\n",
      "('many',)\n",
      "('defeats',)\n",
      "('in',)\n",
      "('life,',)\n",
      "('but',)\n",
      "('never',)\n",
      "('let',)\n",
      "('yourself',)\n",
      "('be',)\n",
      "('defeated.',)\n"
     ]
    }
   ],
   "source": [
    "# Unigram \n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "n = 1\n",
    "sentence = 'You will face many defeats in life, but never let yourself be defeated.'\n",
    "unigrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for item in unigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52c44216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'purpose')\n",
      "('purpose', 'of')\n",
      "('of', 'our')\n",
      "('our', 'life')\n",
      "('life', 'is')\n",
      "('is', 'to')\n",
      "('to', 'happy')\n"
     ]
    }
   ],
   "source": [
    "# Bigram\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "n = 2\n",
    "sentence = 'The purpose of our life is to happy'\n",
    "bigrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for item in bigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b1565aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Whoever', 'is', 'happy')\n",
      "('is', 'happy', 'will')\n",
      "('happy', 'will', 'make')\n",
      "('will', 'make', 'others')\n",
      "('make', 'others', 'happy')\n",
      "('others', 'happy', 'too')\n"
     ]
    }
   ],
   "source": [
    "# Trigram\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "n = 3\n",
    "sentence = 'Whoever is happy will make others happy too'\n",
    "trigrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for item in trigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b718e1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "136: ('oo',)\n",
      "105: ('ku',)\n",
      "90: ('uu',)\n",
      "67: ('ka',)\n",
      "66: ('soo',)\n",
      "63: ('u',)\n",
      "41: ('yidhi',)\n",
      "41: ('ayuu',)\n",
      "39: ('ee',)\n",
      "38: ('dheeg',)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Calculate top term frequencies for a corpus of documents.\"\"\"\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    corpus = load_data('Sheekooyin')\n",
    "\n",
    "    # Compute n-grams\n",
    "    ngrams = Counter(nltk.ngrams(corpus, 1))\n",
    "\n",
    "    # Print most common n-grams\n",
    "    for ngram, freq in ngrams.most_common(10):\n",
    "        print(f\"{freq}: {ngram}\")\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "    contents = []\n",
    "\n",
    "    # Read all files and extract words\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            contents.extend([\n",
    "                word.lower() for word in\n",
    "                nltk.word_tokenize(f.read())\n",
    "                if any(c.isalpha() for c in word)\n",
    "            ])\n",
    "    return contents\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a11f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
